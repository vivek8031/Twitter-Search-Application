{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import pymongo\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(filename):\n",
    "    \"\"\"\n",
    "    Read a JSON file where each JSON object is separated by a newline.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        json_list = []\n",
    "        json_object = ''\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                json_object += line\n",
    "            else:\n",
    "                if json_object:\n",
    "                    json_list.append(json.loads(json_object))\n",
    "                    json_object = ''\n",
    "        if json_object:\n",
    "            json_list.append(json.loads(json_object))\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(tweet_obj):\n",
    "    \"\"\"Extract hashtags from tweet entities.\"\"\"\n",
    "    return [tag['text'] for tag in tweet_obj['entities']['hashtags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_source(tweet_obj):\n",
    "    \"\"\"Get tweet source from tweet object.\"\"\"\n",
    "    source = tweet_obj['source'].lower()\n",
    "    if 'android' in source:\n",
    "        return 'android'\n",
    "    elif 'iphone' in source:\n",
    "        return 'iphone'\n",
    "    else:\n",
    "        return 'web'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_data(user_obj, user_list, user_dict):\n",
    "    \"\"\"Process user data and update user_list.\"\"\"\n",
    "    selected_keys = ['id', 'screen_name', 'description', 'followers_count', 'created_at']\n",
    "    user_data = {key: user_obj.get(key) for key in selected_keys}\n",
    "\n",
    "    if user_data['id'] not in user_dict:\n",
    "        user_list.append(user_data)\n",
    "        user_dict[user_data['id']] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet_data(tweet_obj, tweet_list, user_list, user_dict, tweet_dict, additional_keys=None):\n",
    "    \"\"\"Process tweet data and update tweet_list.\"\"\"\n",
    "    process_user_data(tweet_obj['user'], user_list, user_dict)\n",
    "\n",
    "    if 'extended_tweet' in tweet_obj:\n",
    "        tweet_obj['text'] = tweet_obj['extended_tweet']['full_text']\n",
    "\n",
    "    tweet_obj['hashtags'] = extract_hashtags(tweet_obj)\n",
    "    tweet_obj['source'] = get_tweet_source(tweet_obj)\n",
    "    tweet_obj['user_id'] = tweet_obj['user']['id']\n",
    "\n",
    "    if tweet_obj.get('in_reply_to_user_id') is not None and tweet_obj.get('in_reply_to_screen_name') is not None:\n",
    "        new_user_obj = {\n",
    "            'id': tweet_obj['in_reply_to_user_id'],\n",
    "            'screen_name': tweet_obj['in_reply_to_screen_name'],\n",
    "            'description': None,\n",
    "            'followers_count': None,\n",
    "            'created_at': None\n",
    "        }\n",
    "        process_user_data(new_user_obj, user_list, user_dict)\n",
    "\n",
    "    selected_keys = ['id', 'text', 'created_at', 'source', 'retweet_count', 'favorite_count', 'lang', 'reply_count', 'user_id', 'hashtags']\n",
    "    if additional_keys:\n",
    "        selected_keys += additional_keys\n",
    "\n",
    "    new_tweet = {key: tweet_obj.get(key) for key in selected_keys}\n",
    "\n",
    "    if new_tweet['id'] not in tweet_dict:\n",
    "        tweet_list.append(new_tweet)\n",
    "        tweet_dict[new_tweet['id']] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tweets(tweets):\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON is in a top-level dictionary. \"\"\"\n",
    "    tweets_list = []\n",
    "    retweets_list = []\n",
    "    quotes_list = []\n",
    "    replies_list = []\n",
    "    user_list = []\n",
    "    user_dict = {}\n",
    "    tweet_dict = {}\n",
    "\n",
    "    for tweet_obj in tqdm(tweets, desc=\"Processing tweets\"):\n",
    "        if tweet_obj['in_reply_to_status_id'] is not None:\n",
    "            process_tweet_data(tweet_obj, replies_list, user_list, user_dict, tweet_dict, additional_keys=['in_reply_to_status_id', 'in_reply_to_user_id', 'in_reply_to_screen_name'])\n",
    "\n",
    "        # Check for retweeted_status\n",
    "        is_retweet = 'retweeted_status' in tweet_obj\n",
    "        if is_retweet:\n",
    "            tweet_obj['retweeted_status_id'] = tweet_obj['retweeted_status']['id']\n",
    "            tweet_obj['retweeted_status_user_id'] = tweet_obj['retweeted_status']['user']['id']\n",
    "            tweet_obj['retweeted_status_screen_name'] = tweet_obj['retweeted_status']['user']['screen_name']\n",
    "            process_tweet_data(tweet_obj, retweets_list, user_list, user_dict, tweet_dict, additional_keys=['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_screen_name'])\n",
    "            process_tweet_data(tweet_obj['retweeted_status'], tweets_list, user_list, user_dict, tweet_dict)\n",
    "\n",
    "        # Check for quoted_status\n",
    "        is_quote = 'quoted_status' in tweet_obj\n",
    "        if is_quote:\n",
    "            tweet_obj['quoted_status_id'] = tweet_obj['quoted_status']['id']\n",
    "            tweet_obj['quoted_status_user_id'] = tweet_obj['quoted_status']['user']['id']\n",
    "            tweet_obj['quoted_status_screen_name'] = tweet_obj['quoted_status']['user']['screen_name']\n",
    "            process_tweet_data(tweet_obj, quotes_list, user_list, user_dict, tweet_dict, additional_keys=['quoted_status_id', 'quoted_status_user_id', 'quoted_status_screen_name'])\n",
    "            process_tweet_data(tweet_obj['quoted_status'], tweets_list, user_list, user_dict, tweet_dict)\n",
    "\n",
    "        if not (is_retweet or is_quote or tweet_obj['in_reply_to_status_id']):\n",
    "            process_tweet_data(tweet_obj, tweets_list, user_list, user_dict, tweet_dict)\n",
    "\n",
    "    return tweets_list, retweets_list, quotes_list, replies_list, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_edt_and_epoch(tweets_list):\n",
    "    eastern_tz = pytz.timezone(\"US/Eastern\")\n",
    "    \n",
    "    for tweet in tweets_list:\n",
    "        created_at = tweet.get('created_at')\n",
    "        \n",
    "        if created_at is not None:\n",
    "            created_at_dt = datetime.strptime(created_at, '%a %b %d %H:%M:%S %z %Y')\n",
    "            \n",
    "            created_at_edt = created_at_dt.astimezone(eastern_tz)\n",
    "            tweet['created_at_edt'] = created_at_edt.strftime('%a %b %d %H:%M:%S %Z %Y')\n",
    "            tweet['created_at_epoch'] = int(created_at_dt.timestamp())\n",
    "        else:\n",
    "            tweet['created_at_edt'] = None\n",
    "            tweet['created_at_epoch'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tweets: 100%|██████████| 120434/120434 [00:03<00:00, 30643.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets_list: 41529, retweets_list: 72244, quotes_list: 8696, replies_list: 16132, user_list: 117795\n",
      "tweets_data: 120434\n"
     ]
    }
   ],
   "source": [
    "# Read and process JSON files\n",
    "tweets_data_1 = read_json_file(\"../../data/corona-out-2\")\n",
    "tweets_data_2 = read_json_file(\"../../data/corona-out-3\")\n",
    "tweets_data = tweets_data_1 + tweets_data_2\n",
    "\n",
    "tweets_list, retweets_list, quotes_list, replies_list, user_list = flatten_tweets(tweets_data)\n",
    "\n",
    "# Convert timestamps\n",
    "convert_to_edt_and_epoch(tweets_list)\n",
    "convert_to_edt_and_epoch(retweets_list)\n",
    "convert_to_edt_and_epoch(quotes_list)\n",
    "convert_to_edt_and_epoch(replies_list)\n",
    "convert_to_edt_and_epoch(user_list)\n",
    "\n",
    "# Print lengths of lists\n",
    "print(f\"tweets_list: {len(tweets_list)}, retweets_list: {len(retweets_list)}, quotes_list: {len(quotes_list)}, replies_list: {len(replies_list)}, user_list: {len(user_list)}\")\n",
    "print(f\"tweets_data: {len(tweets_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted collection: tweets_collection\n",
      "Deleted collection: retweets_collection\n",
      "Deleted collection: replies_collection\n",
      "Deleted collection: quotes_collection\n",
      "Created collection: tweets_collection\n",
      "Created collection: retweets_collection\n",
      "Created collection: replies_collection\n",
      "Created collection: quotes_collection\n"
     ]
    }
   ],
   "source": [
    "# Establish a connection to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Select the database\n",
    "db = client[\"twitter_database\"]\n",
    "\n",
    "# Define the collection names\n",
    "collections = [\"tweets_collection\", \"retweets_collection\", \"replies_collection\", \"quotes_collection\"]\n",
    "\n",
    "# Delete the collections\n",
    "for collection_name in collections:\n",
    "    if collection_name in db.list_collection_names():\n",
    "        # If the collection exists, drop it\n",
    "        db[collection_name].drop()\n",
    "        print(f\"Deleted collection: {collection_name}\")\n",
    "    else:\n",
    "        print(f\"Collection not found: {collection_name}\")\n",
    "\n",
    "# Create or load the collections\n",
    "collections_dict = {}\n",
    "for collection_name in collections:\n",
    "    if collection_name in db.list_collection_names():\n",
    "        # If the collection exists, load it\n",
    "        collection = db.get_collection(collection_name)\n",
    "        print(f\"Loaded collection: {collection_name}\")\n",
    "    else:\n",
    "        # If the collection does not exist, create it\n",
    "        collection = db.create_collection(collection_name)\n",
    "        print(f\"Created collection: {collection_name}\")\n",
    "\n",
    "    collections_dict[collection_name] = collection\n",
    "\n",
    "tweets_collection = collections_dict[\"tweets_collection\"]\n",
    "retweets_collection = collections_dict[\"retweets_collection\"]\n",
    "replies_collection = collections_dict[\"replies_collection\"]\n",
    "quotes_collection = collections_dict[\"quotes_collection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting tweets: 100%|\u001b[32m██████████\u001b[0m| 41529/41529 [00:17]\n",
      "Inserting retweets: 100%|\u001b[32m██████████\u001b[0m| 72244/72244 [00:30]\n",
      "Inserting quotes: 100%|\u001b[32m██████████\u001b[0m| 8696/8696 [00:03]\n",
      "Inserting replies: 100%|\u001b[32m██████████\u001b[0m| 16132/16132 [00:06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data insertion completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Insert the values into the MongoDB collections one by one\n",
    "\n",
    "# Insert the values into the MongoDB collections one by one with progress bars\n",
    "for tweet in tqdm(tweets_list, desc=\"Inserting tweets\", colour=\"green\", bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}]\"):\n",
    "    tweets_collection.insert_one(tweet)\n",
    "\n",
    "for retweet in tqdm(retweets_list, desc=\"Inserting retweets\", colour=\"green\", bar_format=\"{l_bar}{bar:10}| {n_fmt}/{total_fmt} [{elapsed}]\"):\n",
    "    retweets_collection.insert_one(retweet)\n",
    "\n",
    "for quote in tqdm(quotes_list, desc=\"Inserting quotes\", colour=\"green\", bar_format=\"{l_bar}{bar:10}| {n_fmt}/{total_fmt} [{elapsed}]\"):\n",
    "    quotes_collection.insert_one(quote)\n",
    "\n",
    "for reply in tqdm(replies_list, desc=\"Inserting replies\", colour=\"green\", bar_format=\"{l_bar}{bar:10}| {n_fmt}/{total_fmt} [{elapsed}]\"):\n",
    "    replies_collection.insert_one(reply)\n",
    "\n",
    "print(\"Data insertion completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to MySQL without specifying the database\n",
    "mysql_connection = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"team6\"\n",
    ")\n",
    "\n",
    "# Check if the twitter_database exists and create it if not\n",
    "cursor = mysql_connection.cursor()\n",
    "cursor.execute(\"CREATE DATABASE IF NOT EXISTS twitter_database;\")\n",
    "mysql_connection.commit()\n",
    "\n",
    "# Close the initial connection\n",
    "cursor.close()\n",
    "mysql_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting users: 100%|\u001b[32m██████████\u001b[0m| 117795/117795 [01:47]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User data insertion completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Establish a connection to MySQL again, this time specifying the database\n",
    "mysql_connection = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"team6\",\n",
    "    database=\"twitter_database\"\n",
    ")\n",
    "\n",
    "# Create the users table if it doesn't exist\n",
    "create_users_table_query = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    id BIGINT PRIMARY KEY,\n",
    "    screen_name VARCHAR(255) NOT NULL,\n",
    "    description TEXT,\n",
    "    followers_count INT,\n",
    "    created_at VARCHAR(255),\n",
    "    created_at_edt VARCHAR(255),\n",
    "    created_at_epoch INT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "cursor = mysql_connection.cursor()\n",
    "cursor.execute(\"DROP TABLE IF EXISTS users;\")\n",
    "cursor.execute(create_users_table_query)\n",
    "mysql_connection.commit()\n",
    "\n",
    "# Function to insert user data one by one into the MySQL database\n",
    "def insert_user_data(user):\n",
    "    insert_user_query = \"\"\"\n",
    "    INSERT INTO users (id, screen_name, description, followers_count, created_at, created_at_edt, created_at_epoch)\n",
    "    VALUES (%(id)s, %(screen_name)s, %(description)s, %(followers_count)s, %(created_at)s, %(created_at_edt)s, %(created_at_epoch)s)\n",
    "    ON DUPLICATE KEY UPDATE\n",
    "        screen_name = %(screen_name)s,\n",
    "        description = %(description)s,\n",
    "        followers_count = %(followers_count)s,\n",
    "        created_at = %(created_at)s,\n",
    "        created_at_edt = %(created_at_edt)s,\n",
    "        created_at_epoch = %(created_at_epoch)s;\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(insert_user_query, user)\n",
    "    mysql_connection.commit()\n",
    "\n",
    "# Insert the user data into the MySQL database one by one with progress bars\n",
    "for user in tqdm(user_list, desc=\"Inserting users\", colour=\"green\", bar_format=\"{l_bar}{bar:10}| {n_fmt}/{total_fmt} [{elapsed}]\"):\n",
    "    insert_user_data(user)\n",
    "\n",
    "# Close the MySQL connection\n",
    "cursor.close()\n",
    "mysql_connection.close()\n",
    "\n",
    "print(\"User data insertion completed.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Indexes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL Indexes:\n",
    "\n",
    "1.  For `get_user_info(user_id)` query:\n",
    "\n",
    "*   Index on `id` field to quickly search users by their ID:\n",
    "\n",
    "`CREATE INDEX idx_users_id ON users (id);`\n",
    "\n",
    "========================================================================================================================================================================\n",
    "\n",
    "\n",
    "2.  For `get_all_users(search_string, date_range)` query:\n",
    "\n",
    "*   Index on `screen_name`, `created_at_epoch`, and `followers_count` fields to speed up search, date range filtering, and sorting by followers count:\n",
    "\n",
    "`CREATE INDEX idx_users_screen_name_created_at_epoch_followers_count ON users (screen_name, created_at_epoch, followers_count);`\n",
    "\n",
    "========================================================================================================================================================================\n",
    "\n",
    "\n",
    "3.  For `get_top_users_by_followers(limit=10)` query:\n",
    "\n",
    "*   Index on `followers_count` field to quickly sort users by followers count:\n",
    "\n",
    "`CREATE INDEX idx_users_followers_count ON users (followers_count);`\n",
    "\n",
    "\n",
    "## MongoDB Indexes:\n",
    "\n",
    "1.  For `get_user_tweets(user_id, collection)` query:\n",
    "\n",
    "*   Index on `user_id` and `created_at_edt` fields to efficiently filter and sort tweets by user ID and creation date:\n",
    "\n",
    "`db.{collection_name}.createIndex({user_id: 1, created_at_edt: -1})`\n",
    "\n",
    "========================================================================================================================================================================\n",
    "\n",
    "2.  For `display_collection(collection, search_string, date_range)` query:\n",
    "\n",
    "*   Index on `text`, `created_at_epoch`, and `retweet_count` fields for efficient text search, date range filtering, and sorting by retweet count:\n",
    "\n",
    "`db.{collection_name}.createIndex({text: \"text\", created_at_epoch: 1, retweet_count: -1})`\n",
    "\n",
    "========================================================================================================================================================================\n",
    "\n",
    "\n",
    "3.  For `get_top_tweets_by_metric(metric, limit=10)` query:\n",
    "\n",
    "*   Index on the metric field (e.g., `retweet_count`) to quickly sort tweets by the metric:\n",
    "\n",
    "`db.{collection_name}.createIndex({{metric}: -1})`\n",
    "\n",
    "========================================================================================================================================================================\n",
    "\n",
    "\n",
    "4.  For `display_top_hashtags(limit=10)` query:\n",
    "\n",
    "*   Index on `hashtags` field to speed up aggregation of hashtags:\n",
    "\n",
    "`db.{collection_name}.createIndex({hashtags: 1})`\n",
    "\n",
    "========================================================================================================================================================================\n",
    "\n",
    "\n",
    "5.  For `display_top_sources(limit=10)` query:\n",
    "\n",
    "*   Index on `source` field to speed up aggregation of sources:\n",
    "\n",
    "`db.{collection_name}.createIndex({source: 1})`\n",
    "\n",
    "========================================================================================================================================================================\n",
    "\n",
    "\n",
    "6.  For `display_hashtags(collection, search_string, date_range)` query:\n",
    "\n",
    "*   Index on `hashtags`, `created_at_epoch`, and `retweet_count` fields for efficient hashtag search, date range filtering, and sorting by retweet count:\n",
    "\n",
    "`db.{collection_name}.createIndex({hashtags: 1, created_at_epoch: 1, retweet_count: -1})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MySQL indexes\n",
    "mysql_connection = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"team6\",\n",
    "    database=\"twitter_database\"\n",
    ")\n",
    "cursor = mysql_connection.cursor()\n",
    "\n",
    "mysql_indexes = [\n",
    "    (\"idx_users_id\", \"id\"),\n",
    "    (\"idx_users_screen_name_created_at_epoch_followers_count\", \"screen_name, created_at_epoch, followers_count\"),\n",
    "    (\"idx_users_followers_count\", \"followers_count\")\n",
    "]\n",
    "\n",
    "for index_name, fields in mysql_indexes:\n",
    "    query = f\"CREATE INDEX {index_name} ON users ({fields})\"\n",
    "    cursor.execute(query)\n",
    "\n",
    "mysql_connection.commit()\n",
    "\n",
    "# Close the MySQL connection\n",
    "cursor.close()\n",
    "mysql_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create indexes for each collection\n",
    "tweets_collection.create_index([(\"user_id\", 1), (\"created_at_edt\", -1)])\n",
    "quotes_collection.create_index([(\"user_id\", 1), (\"created_at_edt\", -1)])\n",
    "replies_collection.create_index([(\"user_id\", 1), (\"created_at_edt\", -1)])\n",
    "retweets_collection.create_index([(\"user_id\", 1), (\"created_at_edt\", -1)])\n",
    "\n",
    "tweets_collection.create_index([(\"text\", \"text\"), (\"created_at_epoch\", 1), (\"retweet_count\", -1)])\n",
    "quotes_collection.create_index([(\"text\", \"text\"), (\"created_at_epoch\", 1), (\"retweet_count\", -1)])\n",
    "replies_collection.create_index([(\"text\", \"text\"), (\"created_at_epoch\", 1), (\"retweet_count\", -1)])\n",
    "retweets_collection.create_index([(\"text\", \"text\"), (\"created_at_epoch\", 1), (\"retweet_count\", -1)])\n",
    "\n",
    "tweets_collection.create_index([(\"retweet_count\", -1)])\n",
    "tweets_collection.create_index([(\"favorite_count\", -1)])\n",
    "\n",
    "tweets_collection.create_index([(\"hashtags\", 1)])\n",
    "\n",
    "tweets_collection.create_index([(\"source\", 1)])\n",
    "\n",
    "tweets_collection.create_index([(\"hashtags\", 1), (\"created_at_epoch\", 1), (\"retweet_count\", -1)])\n",
    "\n",
    "print(\"Indexes created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
